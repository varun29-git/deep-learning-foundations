{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnN7CKiH1sBlLQfKk2FaGS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varun29-git/deep-learning-foundations/blob/main/MLP_in_2_Ways.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a Deep MLP: Batch Normalization & Modular Layers**\n",
        "\n",
        "\n",
        "\n",
        "This notebook explores the internal mechanics of deep neural networks by building a character-level language model from scratch. We progress from a manual, mathematically explicit implementation to a scalable, Object-Oriented architecture mimicking torch.nn.\n",
        "\n",
        "*The notebook builds on following concepts:*\n",
        "\n",
        "**Batch Normalization:** Understanding the math behind population statistics and momentum.\n",
        "\n",
        "**Initialization:** Implementing Kaiming Init ($Gain / \\sqrt{fan_{in}}$) to solve vanishing gradients.\n",
        "\n",
        "**Refactoring:** Moving from global variables to a modular Class-based structure (Linear, BatchNorm1d, Tanh).\n"
      ],
      "metadata": {
        "id": "Rni23Ei7KDkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "kTiGIPiSohr7"
      },
      "outputs": [],
      "source": [
        "# Requirements\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "id": "9JNHlxnAopsN"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "block_size = 3\n",
        "batch_size = 32\n",
        "epsilon = 1e-5"
      ],
      "metadata": {
        "id": "acxcI5kApxHv"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "with open('names.txt', 'r') as f:\n",
        "    names = f.read().splitlines()\n",
        "print(f\"Dataset size: {len(names)}\")\n",
        "\n",
        "# Shuffle the names\n",
        "random.seed(42)\n",
        "random.shuffle(names)\n",
        "\n",
        "alphabets = sorted({ch for name in names for ch in name})\n",
        "\n",
        "# String to Integer\n",
        "stoi = {s:i+1 for i,s in enumerate(alphabets)}\n",
        "stoi['.'] = 0\n",
        "\n",
        "# Integer to String\n",
        "itos = {i:s for s, i in stoi.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTZUb9UsouTF",
        "outputId": "10a4c9e1-acd9-42e3-a8d5-ad7674d43637"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 32033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of Training, Validation and Testing Dataset\n",
        "def build_dataset(words):\n",
        "  X = []\n",
        "  Y = []\n",
        "  for word in words:\n",
        "    context = [0] * block_size\n",
        "    for char in word + '.':\n",
        "      ix = stoi[char]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix]\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(f\"X.shape = {X.shape}, Y.shape = {Y.shape})\")\n",
        "  return X, Y\n",
        "\n",
        "n1 = int(len(names) * 0.8)\n",
        "n2 = int(len(names) * 0.9)\n",
        "\n",
        "Xtr,Ytr = build_dataset(names[:n1]) # Training\n",
        "Xval,Yval = build_dataset(names[n1:n2]) # Validation\n",
        "Xte, Yte = build_dataset(names[n2:]) # Testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz0qPODVo2gi",
        "outputId": "b83cdb97-071b-4599-c04f-f3bd24ba1e15"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape = torch.Size([182625, 3]), Y.shape = torch.Size([182625]))\n",
            "X.shape = torch.Size([22655, 3]), Y.shape = torch.Size([22655]))\n",
            "X.shape = torch.Size([22866, 3]), Y.shape = torch.Size([22866]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "torch.manual_seed(42)\n",
        "vocab_size = len(itos)\n",
        "n_hidden = 100 # Number of Hidden Layers\n",
        "n_emb = 10 # Number of embeddings\n",
        "C = torch.randn((vocab_size, n_emb), generator=g) # Creation of an Embedding table"
      ],
      "metadata": {
        "id": "drs12KvNv5Ed"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. The Manual Approach (First Principles)**\n",
        "\n",
        "This implementation constructs an MLP using a flat script where every operation (like $h_{pre} = Wx + b$) is explicit and parameters are managed globally. This method exposes the raw mechanics of Kaiming Initialization and Batch Normalization without layers of abstraction, ideal for understanding the underlying math.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-Ke5IpUC4BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "W1 = torch.randn((n_emb * block_size, n_hidden), generator=g) * ((5/3) / ((n_emb * block_size) ** 0.5))\n",
        "# W1 divided by sqr root of fan in to make the model less confidently wrong.\n",
        "# Also scaled by 5/3 to prevent from getting into a vanishing gradient problem\n",
        "# Because of batchnorm mean substraction, bias for the linear layer will have no effect. Therefore it is not initialized\n",
        "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
        "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
        "bnbias = torch.ones((1, n_hidden)) * 0.1\n",
        "bnmean_running = torch.zeros((1, n_hidden))\n",
        "bnvar_running = torch.ones((1, n_hidden))\n",
        "\n",
        "parameters = [W1, W2, C, b2, bnbias, bngain]\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "Kck8Wvdfytbx"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training\n",
        "epochs = 100000\n",
        "\n",
        "# Start\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Gradient Initialization\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  # Minibatch Construct\n",
        "  ix = torch.randint(0,Xtr.shape[0], (batch_size,))\n",
        "\n",
        "  # Forward Pass begins\n",
        "  emb = C[Xtr[ix]]\n",
        "  emb_cat = emb.view(emb.shape[0], -1) # Makes Matrix Multiplication possible\n",
        "\n",
        "  # Linear Layer\n",
        "  hpreact = emb_cat @ W1\n",
        "\n",
        "  # Batch Normalisation\n",
        "  bnmeani = hpreact.mean(0, keepdim=True)\n",
        "  bnvari = hpreact.var(0, keepdim=True, unbiased=False)\n",
        "  hpreact = bngain * ((hpreact - bnmeani) / torch.sqrt(bnvari + epsilon)) + bnbias\n",
        "  # Norm = (X - Mean(X)) / Standard Deviation (X)\n",
        "  with torch.no_grad():\n",
        "    bnmean_running = 0.9 * bnmean_running + 0.1 * bnmeani\n",
        "    bnvar_running = 0.9 * bnvar_running + 0.1 * bnvari\n",
        "\n",
        "  # Activation\n",
        "  h = torch.tanh(hpreact)\n",
        "  logits = h @ W2 + b2\n",
        "\n",
        "  # Loss\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  lossi.append(loss.log10().item())\n",
        "  # Backward Pass\n",
        "  loss.backward()\n",
        "\n",
        "  # Optimization\n",
        "  learning_rate = 0.1 if epoch <= (epochs/2) else 0.01\n",
        "  with torch.no_grad():\n",
        "    for p in parameters:\n",
        "      p -= learning_rate * p.grad\n",
        "\n",
        "  # Print Loss\n",
        "  if epoch % (epochs/10) == 0:\n",
        "    print(f\"epoch: {epoch}, loss: {loss.item()}\")\n",
        "\n",
        "# Final Loss\n",
        "print(f\"epoch: {epochs}, loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "rWbK1Mo41bFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe7f635-d136-4f2a-a3a8-606c839dd3e6"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss: 3.5724077224731445\n",
            "epoch: 10000, loss: 2.260617733001709\n",
            "epoch: 20000, loss: 2.3816561698913574\n",
            "epoch: 30000, loss: 2.249138116836548\n",
            "epoch: 40000, loss: 2.1082563400268555\n",
            "epoch: 50000, loss: 2.3121681213378906\n",
            "epoch: 60000, loss: 2.2469069957733154\n",
            "epoch: 70000, loss: 2.449575901031494\n",
            "epoch: 80000, loss: 1.9596929550170898\n",
            "epoch: 90000, loss: 2.2607407569885254\n",
            "epoch: 100000, loss: 1.919772744178772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "\n",
        "# Calculate the complete batch loss\n",
        "def split_loss(split):\n",
        "  x, y = {\n",
        "      \"train\" : (Xtr, Ytr),\n",
        "      \"val\" : (Xval, Yval),\n",
        "      \"test\" : (Xte, Yte),\n",
        "  }[split]\n",
        "\n",
        "  # Forward Pass\n",
        "  emb = C[x]\n",
        "  emb_cat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = (emb_cat) @ W1\n",
        "\n",
        "  # Batch Normalisation\n",
        "  hpreact = bngain * (hpreact - bnmean_running) / torch.sqrt(bnvar_running + epsilon) + bnbias\n",
        "\n",
        "  # Activation\n",
        "  h = torch.tanh(hpreact)\n",
        "  logits = h @ W2 + b2\n",
        "\n",
        "  # Loss\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split,\":\", loss.item())\n",
        "\n",
        "# Prints Actual Losses\n",
        "split_loss('train')\n",
        "split_loss('val')\n",
        "split_loss('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hbDjULIPnbL",
        "outputId": "ecb5603f-d8c3-4205-c9bc-5c02d5ac4e04"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 2.1354637145996094\n",
            "val : 2.154210090637207\n",
            "test : 2.15461802482605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    out = []\n",
        "    context = [0] * block_size\n",
        "\n",
        "    while True:\n",
        "        # Forward Pass Begins\n",
        "\n",
        "\n",
        "        emb = C[torch.tensor([context])] # (1, block_size, n_emb)\n",
        "        emb_cat = emb.view(1, -1)        # (1, block_size * n_emb)\n",
        "\n",
        "        # Linear Layer\n",
        "        # We don't use b1 here because BatchNorm cancels it out\n",
        "        hpreact = emb_cat @ W1\n",
        "\n",
        "        # Batch Normalization\n",
        "        bn_std = torch.sqrt(bnvar_running + epsilon)\n",
        "        hpreact = bngain * (hpreact - bnmean_running) / bn_std + bnbias\n",
        "\n",
        "        # Activation\n",
        "        h = torch.tanh(hpreact)\n",
        "\n",
        "        # Output Layer\n",
        "        logits = h @ W2 + b2\n",
        "\n",
        "        # Sampling\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "\n",
        "        # Shift context\n",
        "        context = context[1:] + [ix]\n",
        "        out.append(ix)\n",
        "\n",
        "        if ix == 0:\n",
        "            break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji9-ENzCERGu",
        "outputId": "c955f7db-38c4-4216-a584-e44f8417cbee"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "carmah.\n",
            "amelle.\n",
            "khi.\n",
            "mili.\n",
            "taty.\n",
            "salaysleer.\n",
            "hubeddelynn.\n",
            "jareei.\n",
            "ner.\n",
            "kia.\n",
            "chaiivon.\n",
            "leigh.\n",
            "ham.\n",
            "jord.\n",
            "quinn.\n",
            "shoilea.\n",
            "jadiquin.\n",
            "ell.\n",
            "dearisia.\n",
            "kael.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. The OOP Approach (Modular Architecture)**\n",
        "\n",
        "This section refactors the code into modular classes mimicking torch.nn to support deep, scalable architectures. It automates parameter management and handles critical state-switching for Batch Normalization, ensuring correct statistics usage during both training and inference."
      ],
      "metadata": {
        "id": "GZ6mrtCqrP_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in,fan_out ,bias=True):\n",
        "    # Wrap weights and biases in torch.nn.Parameter\n",
        "    self.weight = torch.nn.Parameter(torch.randn((fan_in, fan_out), generator=g) * (fan_in ** -0.5))\n",
        "    self.bias = torch.nn.Parameter(torch.zeros(fan_out)) if bias else None\n",
        "\n",
        "  def __call__(self,x):\n",
        "    self.out = x  @ self.weight\n",
        "    if self.bias is not None:\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([self.bias] if self.bias is not None else [])\n",
        "\n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # Parameters trained with backprop, wrapped in torch.nn.Parameter\n",
        "    self.gamma = torch.nn.Parameter(torch.ones(dim))\n",
        "    self.beta = torch.nn.Parameter(torch.zeros(dim))\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True)\n",
        "      xvar = x.var(0, keepdim=True, unbiased=False)\n",
        "    else:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "    xhat = (x - xmean)/torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "class Tanh:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return []"
      ],
      "metadata": {
        "id": "UVFRD6dSrR2C"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters (just put for reference again) :)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "vocab_size = len(itos)\n",
        "n_hidden = 100 # Number of Hidden Layers\n",
        "n_emb = 10 # Number of embeddings\n",
        "C = torch.randn((vocab_size, n_emb), generator=g) # Creation of an Embedding table"
      ],
      "metadata": {
        "id": "g5Spiw1M665y"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C = torch.randn((vocab_size, n_emb), generator=g, requires_grad=True)\n",
        "layers = [\n",
        "    Linear(n_emb * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden),Tanh(),\n",
        "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
        "    Linear(n_hidden, vocab_size),\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Last layer: make less confident\n",
        "    layers[-1].weight *= 0.1\n",
        "    # All other layers: apply gain\n",
        "    for layer in layers[:-1]:\n",
        "        if isinstance(layer, Linear):\n",
        "            layer.weight *= 5/3\n",
        "\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(\"Number of Parameters:\",sum(p.nelement() for p in parameters))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hAU0FnsU7JSs",
        "outputId": "b7711451-7e6e-4cd7-af23-c46f9d5ef158"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Parameters: 46997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "\n",
        "lossi = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X, Y\n",
        "\n",
        "    # Forward pass\n",
        "    emb = C[Xb]  # embed the characters into vectors\n",
        "    x = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
        "    for layer in layers:\n",
        "        x = layer(x)\n",
        "    loss = F.cross_entropy(x, Yb)  # loss function\n",
        "\n",
        "    # Backward pass\n",
        "    for p in parameters:\n",
        "        p.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # Update\n",
        "    learning_rate = 0.1 if epoch <= (epochs/2) else 0.01\n",
        "    for p in parameters:\n",
        "        p.data += -learning_rate * p.grad\n",
        "\n",
        "    # Print Loss\n",
        "    if epoch % (epochs/10) == 0:\n",
        "      print(f\"epoch: {epoch}, loss: {loss.item()}\")\n",
        "\n",
        "# Final Loss\n",
        "print(f\"epoch: {epochs}, loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "HtjBG9jz8xhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f02afd1-9456-41fd-cec2-74d1e50d5133"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss: 3.3017003536224365\n",
            "epoch: 10000, loss: 2.5791115760803223\n",
            "epoch: 20000, loss: 2.43801212310791\n",
            "epoch: 30000, loss: 2.0418097972869873\n",
            "epoch: 40000, loss: 2.49560284614563\n",
            "epoch: 50000, loss: 2.2687554359436035\n",
            "epoch: 60000, loss: 2.0356671810150146\n",
            "epoch: 70000, loss: 2.061049222946167\n",
            "epoch: 80000, loss: 1.8533865213394165\n",
            "epoch: 90000, loss: 1.455241084098816\n",
            "epoch: 100000, loss: 1.8749793767929077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def split_loss_oop(split):\n",
        "    x, y = {\n",
        "        'train': (Xtr, Ytr),\n",
        "        'val': (Xval, Yval),\n",
        "        'test': (Xte, Yte),\n",
        "    }[split]\n",
        "\n",
        "    # Non Training Mode to use running mean and var\n",
        "    for layer in layers:\n",
        "        if isinstance(layer, BatchNorm1d):\n",
        "            layer.training = False\n",
        "\n",
        "    # Forward Pass\n",
        "    emb = C[x]                      # (N, block_size, n_emb)\n",
        "    x = emb.view(emb.shape[0], -1)  # (N, block_size * n_emb)\n",
        "\n",
        "    for layer in layers:\n",
        "        x = layer(x)\n",
        "\n",
        "    loss = F.cross_entropy(x, y)\n",
        "    print(split,\":\",loss.item())\n",
        "\n",
        "\n",
        "    # Switch back to Training Mode\n",
        "    for layer in layers:\n",
        "        if isinstance(layer, BatchNorm1d):\n",
        "            layer.training = True\n",
        "\n",
        "split_loss_oop('train')\n",
        "split_loss_oop('val')"
      ],
      "metadata": {
        "id": "Xehix1mv888a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692738e7-1c2b-481a-f9d7-d796307e5c88"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 2.042226552963257\n",
            "val : 2.09619402885437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Generation\n",
        "\n",
        "for layer in layers:\n",
        "    if isinstance(layer, BatchNorm1d):\n",
        "        layer.training = False\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "\n",
        "    while True:\n",
        "        # Forward pass\n",
        "\n",
        "        emb = C[torch.tensor([context])]\n",
        "        x = emb.view(1, -1)\n",
        "\n",
        "        for layer in layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        logits = x\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Takes samples\n",
        "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "\n",
        "        # Shift context\n",
        "        context = context[1:] + [ix]\n",
        "        out.append(ix)\n",
        "\n",
        "        if ix == 0:\n",
        "            break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm3XgJ79DgRp",
        "outputId": "635e8863-60f0-4a83-821b-93d7afdf32d3"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "carlah.\n",
            "amelle.\n",
            "khi.\n",
            "milia.\n",
            "atlannah.\n",
            "sky.\n",
            "mahnie.\n",
            "deliah.\n",
            "jareei.\n",
            "nellara.\n",
            "chaiiv.\n",
            "kaleigh.\n",
            "ham.\n",
            "joce.\n",
            "quintin.\n",
            "lilea.\n",
            "jadiquor.\n",
            "elo.\n",
            "dearixi.\n",
            "jaxeenivraylen.\n"
          ]
        }
      ]
    }
  ]
}