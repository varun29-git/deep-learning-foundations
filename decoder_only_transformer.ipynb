{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6shbzJpu3BrBjP30CJHKv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varun29-git/deep-learning-foundations/blob/main/decoder_only_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decoder-only Transformer from Scratch**\n",
        "This project is a character-level Generative Pre-trained Transformer (GPT) implemented in PyTorch. It serves as a capstone for a self-study series, tracking the architectural evolution from Bigram models to modular Transformers with causal self-attention, residual connections, and layer normalization.\n",
        "\n",
        "The model is trained on the TinyShakespeare dataset for autoregressive text generation, prioritizing rigorous tensor shape tracking for clarity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Acknowledgments:** Based on the \"Neural Networks: Zero to Hero\" series by Andrej Karpathy."
      ],
      "metadata": {
        "id": "fdu-ZjKPQVj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Requirements\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "szzIb-zNsAD7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "block_size = 256 # What is the maximum context_length for predictions.\n",
        "batch_size = 64 # How many independent sequences we will be processing in parellel.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 200\n",
        "n_head = 6\n",
        "n_embd = 384\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "jZODH-dJtEGX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lGfShzEonXRY"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n"
      ],
      "metadata": {
        "id": "M5rWuhBFndiD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of Characters: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WBoRjA5nv0L",
        "outputId": "45f51819-64b6-45fe-c7f4-00c35da4f285"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(' '.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v8AqBURn4_j",
        "outputId": "383f8ba3-b7d1-4209-a190-f087fd7dcd9b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "itos = {i:s for i,s in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda s: \"\".join([itos[c] for c in s])\n",
        "x = \"Hello, World\"\n",
        "\n",
        "print(f\"X: {x}\")\n",
        "print(f\"Example for Encoder: {encode(x)}\")\n",
        "print(f\"Example for Decoder: {decode(encode(x))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNSgKBEboelH",
        "outputId": "be34e4c9-cc25-4fc1-c93b-53d3936e4b29"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: Hello, World\n",
            "Example for Encoder: [20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42]\n",
            "Example for Decoder: Hello, World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data converted into integers\n",
        "data = torch.tensor(encode(text), dtype= torch.long)"
      ],
      "metadata": {
        "id": "aukvZnSypa0w"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = (int(0.9 * len(data)))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "mpMkKoVjsT-n"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We only train the dataset on chunks\n",
        "x = train_data[:block_size] # Inputs to the transformer\n",
        "y = train_data[1:block_size + 1] # Target"
      ],
      "metadata": {
        "id": "HEPMJF-vswJ3",
        "collapsed": true
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "# Function to get a batch to train with batch size = 4 and block_size = 8\n",
        "def get_batch(split):\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "# Get training batch\n",
        "xb, yb = get_batch(train_data)\n"
      ],
      "metadata": {
        "id": "4leCfOfalZbV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n"
      ],
      "metadata": {
        "id": "mXC0IP6pAdva"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"One head of self attention\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x) # B, T, C\n",
        "    q = self.query(x) # B, T, C\n",
        "    # Compute affinities\n",
        "    wei = q @ k.transpose(-2,-1) * (C**-0.5) # B, T, C @ B, C, T -> B, T, T\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "    # Aggregation of values(weighted)\n",
        "    v = self.value(x) # (B, T, C)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "wi3aCrJHmSgF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # h(x) is (B, T, C), concat on dim -1 -> (B, T, n_embd)\n",
        "    out = torch.cat([h(x)for h in self.heads], dim =-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "Hd05COxirEgI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n"
      ],
      "metadata": {
        "id": "bnYbSPtsr6FQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln_1 = nn.LayerNorm(n_embd)\n",
        "    self.ln_2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln_1(x))\n",
        "    x = x + self.ffwd(self.ln_2(x))\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "U0NAPQqv5_B8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class DecoderTransformerModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets= None):\n",
        "    B, T = idx.shape\n",
        "    # Calculate logits unconditionally, as it's needed for both training and generation.\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      # If targets are provided, calculate the loss.\n",
        "      # Reshape logits and targets for F.cross_entropy\n",
        "      B, T, C = logits.shape\n",
        "      logits_reshaped = logits.view(B * T, C)\n",
        "      targets_reshaped = targets.view(B * T)\n",
        "      loss = F.cross_entropy(logits_reshaped, targets_reshaped)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # Crop idx to the last block_size tokens\n",
        "      # This is crucial for models with a fixed context window.\n",
        "      idx_cond = idx[:, -block_size:] # The block_size variable is available from previous cells.\n",
        "\n",
        "      # Get predictions\n",
        "      # Note: when calling self(idx_cond), targets is implicitly None,\n",
        "      # so only logits will be computed and returned by the forward method.\n",
        "      logits, _ = self(idx_cond) # Using _ to ignore the loss, as it's None during generation\n",
        "\n",
        "      # Focus only on the last time step\n",
        "      logits = logits[:, -1,:] # Becomes (B, C)\n",
        "\n",
        "      # Apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "      # Sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "      # Append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "    return idx # Move return statement outside the loop to generate all tokens\n",
        "\n",
        "model = DecoderTransformerModel()\n",
        "m = model.to(device)\n",
        "# out,loss = m(xb, yb)\n",
        "# print(loss)\n",
        "# idx = torch.zeros((1,1), dtype=torch.long)\n",
        "# print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "id": "HwmFA9PmlEU1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "uRsTDpf0oSdA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_iters == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step: {iter}, train loss: {losses[\"train\"]:.4f}, val loss: {losses[\"val\"]:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n"
      ],
      "metadata": {
        "id": "MqkE2MXYwhmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74896d44-3206-44d1-9e0f-d55ef7dbfbb5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 4.4749, val loss: 4.4704\n",
            "step: 200, train loss: 2.4368, val loss: 2.4629\n",
            "step: 400, train loss: 2.0129, val loss: 2.0855\n",
            "step: 600, train loss: 1.7589, val loss: 1.8979\n",
            "step: 800, train loss: 1.6108, val loss: 1.7865\n",
            "step: 1000, train loss: 1.5282, val loss: 1.7107\n",
            "step: 1200, train loss: 1.4644, val loss: 1.6582\n",
            "step: 1400, train loss: 1.4207, val loss: 1.6346\n",
            "step: 1600, train loss: 1.3746, val loss: 1.6123\n",
            "step: 1800, train loss: 1.3482, val loss: 1.5899\n",
            "step: 2000, train loss: 1.3165, val loss: 1.5702\n",
            "step: 2200, train loss: 1.2929, val loss: 1.5523\n",
            "step: 2400, train loss: 1.2718, val loss: 1.5485\n",
            "step: 2600, train loss: 1.2545, val loss: 1.5426\n",
            "step: 2800, train loss: 1.2338, val loss: 1.5326\n",
            "step: 3000, train loss: 1.2207, val loss: 1.5345\n",
            "step: 3200, train loss: 1.2036, val loss: 1.5299\n",
            "step: 3400, train loss: 1.1879, val loss: 1.5293\n",
            "step: 3600, train loss: 1.1740, val loss: 1.5235\n",
            "step: 3800, train loss: 1.1597, val loss: 1.5238\n",
            "step: 4000, train loss: 1.1464, val loss: 1.5286\n",
            "step: 4200, train loss: 1.1324, val loss: 1.5301\n",
            "step: 4400, train loss: 1.1202, val loss: 1.5337\n",
            "step: 4600, train loss: 1.1067, val loss: 1.5392\n",
            "step: 4800, train loss: 1.0939, val loss: 1.5349\n",
            "1.220286250114441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(idx, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "id": "p3MLDZ7fxQqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32d39e6-c5d6-4e5e-eb17-231476447753"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "KING RICHARD III:\n",
            "Qow yourself-breating kind and success and instant\n",
            "That cries 'twixt countrusting  look intelloastor.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "My lord, to--\n",
            "\n",
            "KING RICHARD II:\n",
            "Queen, the hoursing\n",
            "Good friends with body butternoons, his grieval,\n",
            "And revel the brother of the hearts are the lie:\n",
            "And if thou canst deserve imperate,\n",
            "Horr ha quest heretors to thee,\n",
            "Whose peace and his wrong must too,\n",
            "Or perform that needy,\n",
            "He flinteth him the nay and chiles in these unterring.\n",
            "A safeguard is of my love.\n",
            "Th\n"
          ]
        }
      ]
    }
  ]
}